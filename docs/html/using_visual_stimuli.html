
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Visual stimuli in Signals</title><meta name="generator" content="MATLAB 9.6"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2020-02-19"><meta name="DC.source" content="using_visual_stimuli.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Visual stimuli in Signals</h1><!--introduction--><p>Signals uses the OpenGL MEX wrapper functions provided by PsychToolbox to render the visual stimuli.</p><p>In order to build up an enviroment we need to know a few things:</p><div><ol><li>Where are the objects (stimuli) with respect to one another in the world</li><li>Where are the objects with respect to the viewer</li><li>How to do these coordinates map to a 2D surface (a screen)</li></ol></div><p>Thus when we define a stimulus in visual space it is transformed by our model into physical space by the model (or world-to-camera) matrix then to projected 2D space by our projection matrix.  These transformations are done in the shader.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">The visual stimulus object</a></li><li><a href="#2">vis.screen</a></li><li><a href="#3">vis.init</a></li><li><a href="#4">Plane projection</a></li><li><a href="#5">The shader</a></li><li><a href="#8">Clip space</a></li><li><a href="#9">NDC space</a></li><li><a href="#10">Rasterization</a></li><li><a href="#11">vis.draw</a></li><li><a href="#12">Viewing model</a></li><li><a href="#13">Layers</a></li><li><a href="#14">Stimuli</a></li><li><a href="#15">Images</a></li><li><a href="#16">Checker / sparse noise</a></li><li><a href="#17">Gabor patch &amp; gratings</a></li><li><a href="#18">Shapes</a></li><li><a href="#19">Dynamic textures</a></li><li><a href="#20">vis.rgba &amp; vis.rgbaFromUint8</a></li><li><a href="#21">Notes</a></li><li><a href="#22">Etc.</a></li></ul></div><h2 id="1">The visual stimulus object</h2><p>The visual stimulus object is the 4th input to an experiment definition. It is a <tt>StructRef</tt> object, which like a structure, can be assigned fields.</p><h2 id="2">vis.screen</h2><p>The model produced by vis.screen is a matrix (known as the world-to-camera matrix) that transforms our world coordinates (visual degrees) to camera coodinates (the physical location of the viewer).  It technically does two transforms in one: object-to-world the world-to-camera, however in our viewing model the the camera IS the world, so the object-to-world matrix is identity.  See also <a href="./hardware_config.html#27">hardware configuration</a>:</p><pre class="codeinput">root = fileparts(which(<span class="string">'addRigboxPaths'</span>));
opentoline(fullfile(root, <span class="string">'docs'</span>, <span class="string">'scripts'</span>, <span class="string">'hardware_config.m'</span>), 344, 1)
</pre><h2 id="3">vis.init</h2><p>Loads the shader (slimshady) and initializes a projection matrix with default viewing parameters.</p><p>The viewing model is 'pseudo-circular'.  An inverted spherical mesh is created using <tt>vis.uniSphereTriangles</tt> onto which all textures are painted.  This is designed to compensate for the fact that the screen edges are further away than the centre when the viewer is facing the middle of the screen.  As textures move further along the azimuth, they enlarge.</p><h2 id="4">Plane projection</h2><p>A 4x4 plane projection matrix when multiplied by a 3D coordinates in camera space gives you the 2D coordinates on the screen/projection surface.  This maxtrix allows us to map 3D coordinates to images that follow the rules of perspective.  Clipping happens here.</p><h2 id="5">The shader</h2><p>This is the job of the vertex shader.  The shader's job is to transform our vertices from camera space (visual degrees) to clip space.  The shader returns 'gl_Position' (an OpenGL global variable) which is the result of multiplying vertex postion by the plane projection and world-to-camera matrices.  The shader applies all nessesary transformations (scaling, rotating and translating).  The shader converts the vertices to homogeneous coordinates (vec4), i.e. <tt>position = [position 1];</tt> See <tt>slimshady.vert</tt></p><pre>[Projection matrix] [World-to-camera matrix]
       \                      /
        \                    /
         \                  /
          \                /
           \              /
            \            /
             \          /
              \        /
-------------------------------------GPU----
                \    /
                 \  /
                  \/
            [Vertex shader]</pre><p>Once in clip space, the fragment shader defines its appearence (i.e. colour).</p><h2 id="8">Clip space</h2><p>After this plane projection we are in clip space.  Here points have homogeneous (4D) coordinates.</p><h2 id="9">NDC space</h2><p>After the projection is divided by Clip.W leaving us in Normalized Device Space (NDC).  The resulting matrix has values between [-1, 1].  Values outside of this range are outside the clipping space.  In OpenGL (c.f. Direct3D) this matrix is cubic.  NDC coordinates are agnostic to screen shape and always [-1, 1].</p><h2 id="10">Rasterization</h2><p>This step uses the view port and depth range to translate everything to fragment locations in screen/window space.  This is done on the GPU.</p><h2 id="11">vis.draw</h2><div><ul><li>vis.init</li><li>vis.screen</li><li>vis.loadLayerTextures</li><li>vis.reloadLayerTextures</li></ul></div><h2 id="12">Viewing model</h2><div><ul><li>vis.planeProjection</li><li>vis.uniSphereTriangles</li><li>vis.quadToTriangles</li></ul></div><h2 id="13">Layers</h2><p>Current layers functions:</p><div><ul><li>gaussianLayer</li><li>circLayer</li><li>rectLayer</li><li>crossLayer</li><li>sinusoidLayer</li><li>squareWaveLayer</li><li>emptyLayer</li></ul></div><pre class="codeinput"><span class="comment">%VIS.EMPTYLAYER Template texture layer for rendering in Signals</span>
<span class="comment">%  Returns a struct of paramters and their defaults used by VIS.DRAW to</span>
<span class="comment">%  load a visual stimulus layer.  If n &gt; 1 a non-scalar struct is returned</span>
<span class="comment">%  of length n (default 1).</span>
<span class="comment">%</span>
<span class="comment">%  TODO Document viewAngle, texAngle and pos</span>
<span class="comment">%  @body There is currently no information on how these three parameters</span>
<span class="comment">%  are used by the viewing model.  For example, what is the practical</span>
<span class="comment">%  difference between `texOffset` and `pos`, or `viewAngle` and `texAngle`?</span>
<span class="comment">%</span>
<span class="comment">%  See also VIS.DRAW, VIS.RGBA</span>

<span class="comment">% Create an empty structure</span>
layer = struct;
<span class="comment">% SHOW a logical indicating whether or not the stimulus is visible</span>
layer.show = false;
<span class="comment">% TEXTUREID a char array used by VIS.DRAW to identify the texture layer.</span>
<span class="comment">% Layers with unique texture data (i.e. the data stored in rgba) must have</span>
<span class="comment">% unique IDs in order to be loaded into the buffer seperately.  Preceeding</span>
<span class="comment">% the ID with '~' indicates that it is a dynamic texture to be loaded anew</span>
<span class="comment">% each time. Dynamic textures are those where the underlying image array</span>
<span class="comment">% changes.</span>
layer.textureId = [];
<span class="comment">% POS</span>
layer.pos = [0 0]';
<span class="comment">% SIZE array of the form [azimuth altitude] defining the size of the</span>
<span class="comment">% texture in visual degrees</span>
layer.size = [0 0]';
<span class="comment">% VIEWANGLE The view angle in degrees</span>
layer.viewAngle = 0;
<span class="comment">% TEXANGLE the texture angle in degrees</span>
layer.texAngle = 0;
<span class="comment">% TEXOFFSET an array of the form [azimuth altitude] indicating the texture</span>
<span class="comment">% offset from the centre of the viewer's visual field in visual degrees</span>
layer.texOffset = [0 0]';
<span class="comment">% ISPERIODIC logical - when true the texture is replicated across the</span>
<span class="comment">% entire visual space</span>
layer.isPeriodic = true;
<span class="comment">% BLENDING char array defining the type of blending used.</span>
<span class="comment">% Options:</span>
<span class="comment">%  'none' (/ ''),</span>
<span class="comment">%  'source' (/ 'src'),</span>
<span class="comment">%  'destination' (/ 'dst'),</span>
<span class="comment">%  '1-source' (/'1-src')</span>
layer.blending = <span class="string">'source'</span>;
<span class="comment">% MINCOLOUR &amp; MAXCOLOUR arrays of the form [R G B A] indicating the min</span>
<span class="comment">% (max) intensity of the red, green and blue channels, along with the amout</span>
<span class="comment">% of opacity (alpha).  Values must be between 0 and 1.</span>
layer.minColour = [0 0 0 0]';
layer.maxColour = [1 1 1 1]';
<span class="comment">% COLOURMASK logical array indicating whether the red, green, blue and</span>
<span class="comment">% alpha channels may be written to the frame buffer.  When any of these</span>
<span class="comment">% channels are set to false no change is made to that component of any</span>
<span class="comment">% pixel in any of the color buffers, regardless of any changes to the</span>
<span class="comment">% texture image</span>
layer.colourMask = [true true true true]';
<span class="comment">% INTERPOLATION char array indicating the type of interpolation applied.</span>
<span class="comment">% Options:</span>
<span class="comment">%  'nearest' - Nearest neighbour interpolation</span>
<span class="comment">%  'linear' - linear interpolation</span>
layer.interpolation = <span class="string">'linear'</span>;
<span class="comment">% RGBA Column array of uint8 RGBA values for each pixel (left to right, top</span>
<span class="comment">% to bottom) in the texture image. The values must be between 0 and 255.</span>
<span class="comment">% For example take a matrix.  See also VIS.RGBA</span>
layer.rgba = [];
<span class="comment">% RGBASIZE array of the form [m n] where m and n are the sizes of the first</span>
<span class="comment">% two dimentions of the texture image</span>
layer.rgbaSize = [0 0]';
</pre><h2 id="14">Stimuli</h2><h2 id="15">Images</h2><p>The function for making image textures is <tt>vis.image</tt>.  Images can be arrays with values between 0-1 (MATLAB-style) or 0-255.  They may be monochromatic ([m,n,1]) or be RGB(A) ([m,n,3-4]).  Images can be loaded in a few different ways.  If you don't intend for the underlying image to change you can pass in a path to the image:</p><pre class="codeinput">srcImg = which(<span class="string">'cell.tif'</span>); <span class="comment">% Path to image</span>
img = vis.image(t, srcImg);
<span class="comment">% The source image may be a MAT file or an image file (tiff, png, etc.)</span>
<span class="comment">% If an alpha layer is present it will used.  This can be overridden by</span>
<span class="comment">% providing an alpha layer as a positional argument:</span>
img = vis.image(t, srcImg, 1); <span class="comment">% alpha may be scalar or array the size of img.</span>
<span class="comment">% If creating more than one visual element (e.g. you have two images you</span>
<span class="comment">% want to show at the same time) and are providing a source path, the names</span>
<span class="comment">% of the image files must be unique.  This is because the file name is used</span>
<span class="comment">% as the texture ID, which is ID used by Signals to distinguish textures.</span>

<span class="comment">% The source image may be loaded separately and passed in the same way:</span>
images = load(<span class="string">'imdemos.mat'</span>);
img = vis.image(t, images.circles);

<span class="comment">% Finally the input may be a Signal whose value is the image array.  When</span>
<span class="comment">% the source image is a Signal it is loaded as a dynamic texture (the</span>
<span class="comment">% layer's textureId field starts with a '~').  This allows the source image</span>
<span class="comment">% to change throughout the experiment, however if you don't intend for your</span>
<span class="comment">% source image to change, consider pre-loading it like the above examples</span>
<span class="comment">% as it is more efficient.</span>

<span class="comment">% You can optionally add a Gaussian window over the image:</span>
img.window = <span class="string">'gauss'</span>;
img.sigma = [10 10];

<span class="comment">% The image position and size may be set as expected:</span>
img.dims = [40 20];
img.orientation = 180; <span class="comment">% upside-down</span>
img.azimuth = 0; <span class="comment">% centred in x</span>
img.altitude = 10; <span class="comment">% slightly elevated</span>

<span class="comment">% The image may also be tiled across the screen by setting the repeat flag:</span>
img.repeat = true; <span class="comment">% cover the whole screen with image tiles</span>
</pre><h2 id="16">Checker / sparse noise</h2><h2 id="17">Gabor patch &amp; gratings</h2><h2 id="18">Shapes</h2><h2 id="19">Dynamic textures</h2><h2 id="20">vis.rgba &amp; vis.rgbaFromUint8</h2><pre class="codeinput"><span class="comment">%VIS.GRATING Returns a Signals grating stimulus defining a grating texture</span>
<span class="comment">%  Produces a visual element for parameterizing the presentation of a</span>
<span class="comment">%  grating. Produces a grating that can be either sinusoidal or</span>
<span class="comment">%  square-wave, and may be windowed by a Gaussian stencil, producing a</span>
<span class="comment">%  Gabor patch.</span>
<span class="comment">%</span>
<span class="comment">%  Inputs:</span>
<span class="comment">%    't' - The "time" signal. Used to obtain the Signals network ID.</span>
<span class="comment">%      (Could be any signal within the network - 't' is chosen by</span>
<span class="comment">%      convention).</span>
<span class="comment">%    'grating' - A char array defining the nature of the grating. Options</span>
<span class="comment">%      are 'sinusoid' (default) or 'squarewave'.</span>
<span class="comment">%    'window' - A char array defining the type of windowing applied.</span>
<span class="comment">%      Options are 'gaussian' (default) or 'none'.</span>
<span class="comment">%</span>
<span class="comment">%  Outputs:</span>
<span class="comment">%    'elem' - a subscriptable signal containing fields which parametrize</span>
<span class="comment">%      the stimulus, and a field containing the processed texture layer.</span>
<span class="comment">%      Any of the fields may be a signal.</span>
<span class="comment">%</span>
<span class="comment">%  Stimulus parameters (fields belonging to 'elem'):</span>
<span class="comment">%    'grating' - see above</span>
<span class="comment">%    'window' - see above</span>
<span class="comment">%    'azimuth' - the azimuth of the image (position of the centre pixel in</span>
<span class="comment">%     visual degrees).  Default 0</span>
<span class="comment">%    'altitude' - the altitude of the image (position of the centre pixel</span>
<span class="comment">%     in visual degrees). Default 0</span>
<span class="comment">%    'sigma' - if window is Gaussian, the size of the window in visual</span>
<span class="comment">%      degrees. Must be an array of the form [width height].</span>
<span class="comment">%      Default [10 10]</span>
<span class="comment">%    'phase' - the phase of the grating in visual degrees.  Default 0</span>
<span class="comment">%    'spatialFreq' - the spatial frequency of the grating in cycles per</span>
<span class="comment">%      visual degree.  Default 1/15</span>
<span class="comment">%    'orientation' - the orientation of the grating in degrees. Default 0</span>
<span class="comment">%    'colour' - an array defining the intensity of the red, green and blue</span>
<span class="comment">%      channels respectively. Values must be between 0 and 1.</span>
<span class="comment">%      Default [1 1 1]</span>
<span class="comment">%    'contrast' - the normalized contrast of the grating (between 0 and 1).</span>
<span class="comment">%      Default 1</span>
<span class="comment">%    'show' - a logical indicating whether or not the stimulus is visible.</span>
<span class="comment">%      Default false</span>
<span class="comment">%</span>
<span class="comment">%  See Also VIS.EMPTYLAYER, VIS.PATCH, VIS.IMAGE, VIS.CHECKER6, VIS.GRID</span>


<span class="comment">% Map the visual element signal through the below function 'makeLayers' and</span>
<span class="comment">% assign it to the 'layers' field.  When any of the above parameters takes</span>
<span class="comment">% a new value, 'makeLayer' is called, returning the texture layer.</span>
<span class="comment">% 'flattenStruct' returns the same texture layer but with all fields</span>
<span class="comment">% containing signals replaced by their current value. The 'layers' field</span>
<span class="comment">% is loaded by VIS.DRAW</span>
</pre><h2 id="21">Notes</h2><div><ol><li>Like MATLAB OpenGl uses column-major order</li><li>Camera space may also be referred to as view space</li><li>In the unit cube, 1 means the object is at the far clipping plane (right up against the back-drop as it were) and -1, the near clipping plane (right up against the screen).  All points visible to the camera have a negative z-component.</li><li>In OpenGL (GLUT more precisely), the FOV corresponds to the vertical angle</li><li><a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/projection-matrix-introduction">https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/projection-matrix-introduction</a></li></ol></div><h2 id="22">Etc.</h2><p>Author: Miles Wells</p><p>v0.0.1</p><pre class="codeinput"><span class="comment">%#ok&lt;*NASGU,*NOPTS&gt;</span>
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2019a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Visual stimuli in Signals
% Signals uses the OpenGL MEX wrapper functions provided by PsychToolbox to
% render the visual stimuli. 
%
% In order to build up an enviroment we need to know a few things:
%
% # Where are the objects (stimuli) with respect to one another in the
% world
% # Where are the objects with respect to the viewer
% # How to do these coordinates map to a 2D surface (a screen)
% 
% Thus when we define a stimulus in visual space it is transformed by our
% model into physical space by the model (or world-to-camera) matrix then
% to projected 2D space by our projection matrix.  These transformations
% are done in the shader.

%% The visual stimulus object
% The visual stimulus object is the 4th input to an experiment definition.
% It is a |StructRef| object, which like a structure, can be assigned
% fields.



%% vis.screen
% The model produced by vis.screen is a matrix (known as the
% world-to-camera matrix) that transforms our world coordinates (visual
% degrees) to camera coodinates (the physical location of the viewer).  It
% technically does two transforms in one: object-to-world the
% world-to-camera, however in our viewing model the the camera IS the
% world, so the object-to-world matrix is identity.  See also
% <./hardware_config.html#27 hardware configuration>:
root = fileparts(which('addRigboxPaths'));
opentoline(fullfile(root, 'docs', 'scripts', 'hardware_config.m'), 344, 1)

%% vis.init
% Loads the shader (slimshady) and initializes a projection matrix with
% default viewing parameters.
%
% The viewing model is 'pseudo-circular'.  An inverted spherical mesh is
% created using |vis.uniSphereTriangles| onto which all textures are
% painted.  This is designed to compensate for the fact that the screen
% edges are further away than the centre when the viewer is facing the
% middle of the screen.  As textures move further along the azimuth, they
% enlarge.

%% Plane projection
% A 4x4 plane projection matrix when multiplied by a 3D coordinates in
% camera space gives you the 2D coordinates on the screen/projection
% surface.  This maxtrix allows us to map 3D coordinates to images that
% follow the rules of perspective.  Clipping happens here.  

%% The shader
% This is the job of the vertex shader.  The shader's job is to transform
% our vertices from camera space (visual degrees) to clip space.  The
% shader returns 'gl_Position' (an OpenGL global variable) which is the
% result of multiplying vertex postion by the plane projection and
% world-to-camera matrices.  The shader applies all nessesary
% transformations (scaling, rotating and translating).  The shader converts
% the vertices to homogeneous coordinates (vec4), i.e. |position =
% [position 1];| See |slimshady.vert|

%%  
%  [Projection matrix] [World-to-camera matrix]
%         \                      /
%          \                    /
%           \                  /
%            \                /
%             \              /
%              \            /
%               \          /
%                \        / 
%  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-GPUREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
%                  \    /
%                   \  /
%                    \/
%              [Vertex shader]

%%
% Once in clip space, the fragment shader defines its appearence (i.e.
% colour).

%% Clip space
% After this plane projection we are in clip space.  Here points have
% homogeneous (4D) coordinates.  

%% NDC space
% After the projection is divided by Clip.W leaving us in Normalized Device
% Space (NDC).  The resulting matrix has values between [-1, 1].  Values
% outside of this range are outside the clipping space.  In OpenGL (c.f.
% Direct3D) this matrix is cubic.  NDC coordinates are agnostic to screen
% shape and always [-1, 1].

%% Rasterization
% This step uses the view port and depth range to translate everything to
% fragment locations in screen/window space.  This is done on the GPU.

%% vis.draw
% * vis.init
% * vis.screen
% * vis.loadLayerTextures
% * vis.reloadLayerTextures

%% Viewing model
% * vis.planeProjection
% * vis.uniSphereTriangles
% * vis.quadToTriangles

%% Layers
% Current layers functions:
%
% * gaussianLayer
% * circLayer
% * rectLayer
% * crossLayer
% * sinusoidLayer
% * squareWaveLayer
% * emptyLayer

%VIS.EMPTYLAYER Template texture layer for rendering in Signals
%  Returns a struct of paramters and their defaults used by VIS.DRAW to
%  load a visual stimulus layer.  If n > 1 a non-scalar struct is returned
%  of length n (default 1).
%
%  TODO Document viewAngle, texAngle and pos
%  @body There is currently no information on how these three parameters
%  are used by the viewing model.  For example, what is the practical
%  difference between `texOffset` and `pos`, or `viewAngle` and `texAngle`?
%
%  See also VIS.DRAW, VIS.RGBA

% Create an empty structure
layer = struct;
% SHOW a logical indicating whether or not the stimulus is visible
layer.show = false;
% TEXTUREID a char array used by VIS.DRAW to identify the texture layer.
% Layers with unique texture data (i.e. the data stored in rgba) must have
% unique IDs in order to be loaded into the buffer seperately.  Preceeding
% the ID with '~' indicates that it is a dynamic texture to be loaded anew
% each time. Dynamic textures are those where the underlying image array
% changes.
layer.textureId = [];
% POS 
layer.pos = [0 0]';
% SIZE array of the form [azimuth altitude] defining the size of the
% texture in visual degrees
layer.size = [0 0]';
% VIEWANGLE The view angle in degrees 
layer.viewAngle = 0;
% TEXANGLE the texture angle in degrees
layer.texAngle = 0;
% TEXOFFSET an array of the form [azimuth altitude] indicating the texture
% offset from the centre of the viewer's visual field in visual degrees
layer.texOffset = [0 0]';
% ISPERIODIC logical - when true the texture is replicated across the
% entire visual space
layer.isPeriodic = true;
% BLENDING char array defining the type of blending used.  
% Options:
%  'none' (/ ''), 
%  'source' (/ 'src'), 
%  'destination' (/ 'dst'), 
%  '1-source' (/'1-src')
layer.blending = 'source';
% MINCOLOUR & MAXCOLOUR arrays of the form [R G B A] indicating the min
% (max) intensity of the red, green and blue channels, along with the amout
% of opacity (alpha).  Values must be between 0 and 1.
layer.minColour = [0 0 0 0]';
layer.maxColour = [1 1 1 1]';
% COLOURMASK logical array indicating whether the red, green, blue and
% alpha channels may be written to the frame buffer.  When any of these
% channels are set to false no change is made to that component of any
% pixel in any of the color buffers, regardless of any changes to the
% texture image
layer.colourMask = [true true true true]';
% INTERPOLATION char array indicating the type of interpolation applied.
% Options:
%  'nearest' - Nearest neighbour interpolation
%  'linear' - linear interpolation
layer.interpolation = 'linear';
% RGBA Column array of uint8 RGBA values for each pixel (left to right, top
% to bottom) in the texture image. The values must be between 0 and 255.
% For example take a matrix.  See also VIS.RGBA
layer.rgba = [];
% RGBASIZE array of the form [m n] where m and n are the sizes of the first
% two dimentions of the texture image
layer.rgbaSize = [0 0]';

%% Stimuli

%%% Images
% The function for making image textures is |vis.image|.  Images can be
% arrays with values between 0-1 (MATLAB-style) or 0-255.  They may be
% monochromatic ([m,n,1]) or be RGB(A) ([m,n,3-4]).  Images can be loaded in a
% few different ways.  If you don't intend for the underlying image to
% change you can pass in a path to the image:
srcImg = which('cell.tif'); % Path to image
img = vis.image(t, srcImg);
% The source image may be a MAT file or an image file (tiff, png, etc.)
% If an alpha layer is present it will used.  This can be overridden by
% providing an alpha layer as a positional argument:
img = vis.image(t, srcImg, 1); % alpha may be scalar or array the size of img.
% If creating more than one visual element (e.g. you have two images you
% want to show at the same time) and are providing a source path, the names
% of the image files must be unique.  This is because the file name is used
% as the texture ID, which is ID used by Signals to distinguish textures.

% The source image may be loaded separately and passed in the same way:
images = load('imdemos.mat');
img = vis.image(t, images.circles);

% Finally the input may be a Signal whose value is the image array.  When
% the source image is a Signal it is loaded as a dynamic texture (the
% layer's textureId field starts with a '~').  This allows the source image
% to change throughout the experiment, however if you don't intend for your
% source image to change, consider pre-loading it like the above examples
% as it is more efficient.

% You can optionally add a Gaussian window over the image:
img.window = 'gauss';
img.sigma = [10 10];

% The image position and size may be set as expected:
img.dims = [40 20];
img.orientation = 180; % upside-down
img.azimuth = 0; % centred in x
img.altitude = 10; % slightly elevated

% The image may also be tiled across the screen by setting the repeat flag:
img.repeat = true; % cover the whole screen with image tiles

%%% Checker / sparse noise

%%% Gabor patch & gratings

%%% Shapes

%% Dynamic textures

%% vis.rgba & vis.rgbaFromUint8
%VIS.GRATING Returns a Signals grating stimulus defining a grating texture
%  Produces a visual element for parameterizing the presentation of a
%  grating. Produces a grating that can be either sinusoidal or
%  square-wave, and may be windowed by a Gaussian stencil, producing a 
%  Gabor patch.
%
%  Inputs:
%    't' - The "time" signal. Used to obtain the Signals network ID.
%      (Could be any signal within the network - 't' is chosen by
%      convention).
%    'grating' - A char array defining the nature of the grating. Options
%      are 'sinusoid' (default) or 'squarewave'.
%    'window' - A char array defining the type of windowing applied.
%      Options are 'gaussian' (default) or 'none'.
%    
%  Outputs:
%    'elem' - a subscriptable signal containing fields which parametrize
%      the stimulus, and a field containing the processed texture layer. 
%      Any of the fields may be a signal.
% 
%  Stimulus parameters (fields belonging to 'elem'):
%    'grating' - see above
%    'window' - see above
%    'azimuth' - the azimuth of the image (position of the centre pixel in 
%     visual degrees).  Default 0
%    'altitude' - the altitude of the image (position of the centre pixel 
%     in visual degrees). Default 0
%    'sigma' - if window is Gaussian, the size of the window in visual 
%      degrees. Must be an array of the form [width height].  
%      Default [10 10]
%    'phase' - the phase of the grating in visual degrees.  Default 0
%    'spatialFreq' - the spatial frequency of the grating in cycles per
%      visual degree.  Default 1/15
%    'orientation' - the orientation of the grating in degrees. Default 0
%    'colour' - an array defining the intensity of the red, green and blue
%      channels respectively. Values must be between 0 and 1.  
%      Default [1 1 1]
%    'contrast' - the normalized contrast of the grating (between 0 and 1).  
%      Default 1
%    'show' - a logical indicating whether or not the stimulus is visible.
%      Default false
%
%  See Also VIS.EMPTYLAYER, VIS.PATCH, VIS.IMAGE, VIS.CHECKER6, VIS.GRID


% Map the visual element signal through the below function 'makeLayers' and
% assign it to the 'layers' field.  When any of the above parameters takes
% a new value, 'makeLayer' is called, returning the texture layer.
% 'flattenStruct' returns the same texture layer but with all fields
% containing signals replaced by their current value. The 'layers' field
% is loaded by VIS.DRAW

%% Notes
% # Like MATLAB OpenGl uses column-major order
% # Camera space may also be referred to as view space
% # In the unit cube, 1 means the object is at the far clipping plane
% (right up against the back-drop as it were) and -1, the near clipping
% plane (right up against the screen).  All points visible to the camera
% have a negative z-component.
% # In OpenGL (GLUT more precisely), the FOV corresponds to the vertical
% angle
% # <https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/projection-matrix-introduction
% https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/projection-matrix-introduction>

%% Etc.
% Author: Miles Wells
%
% v0.0.1
%#ok<*NASGU,*NOPTS>
##### SOURCE END #####
--></body></html>